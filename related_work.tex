%!TEX root=paper.tex
\section{Related Work}\label{sec:related}

\paragraph{Object recognition with CNN}\label{object-recognition-with-cnn}
The recent success of deep convolutional neural networks (CNN) Alexnet \cite{Krizhevsky-NIPS-2012} on image classification tasks such as ImageNet \cite{deng2009cvpr} has spurred various investigations \cite{Girshick-CVPR-2014,Zou-CVPR-2014,Simonyan-ICLR-2014,Sermanet-ICLR-2014} how this success can be carried over to an detection task despite the prohibitive cost of evaluating such models densely in a naive manner. These previous attempts aim at reducing computational cost by reducing the search space of sliding window search by a bottom up window proposal scheme \cite{Girshick-CVPR-2014}, reducing computational overhead by sharing computation between windows to be evaluated \cite{Zou-CVPR-2014,Sermanet-ICLR-2014} as well as deriving a window sampling scheme that is based on the gradient of the network \cite{Simonyan-ICLR-2014}. All these method have in common that they rely on a fixed strategy and cannot adapt the computational scheme based on intermediate results, while we propose a dynamic scheme that steers computation based on intermediate outcomes in order to spend computation time most effectively. In contrast to previous work on shared dense computation layers in CNN classifiers for detection \cite{Sermanet-ICLR-2014}, we maintain effective canonicalization of region via warping and show its importance for achieving high performance.

\paragraph{Cascaded detection}\label{cascaded-detection}
Detection cascades \cite{Viola2004,Felzenszwalb-CVPR-2010} are a well established techniques to condition the computational scheme on intermediate outcomes based on a fixed evaluation order of features/classifiers. This idea has seen several refinements including sharing computation across cascades \cite{Dollar-ECCV-2012} and the possibility to skip evaluations \cite{benbouzid12icml}. Recently, a CNN with a cascaded structure for coarse to fine inference of facial feature points has been proposed \cite{cnn_cascade} as well as a boosted cascade trained on CNN features \cite{Zou-CVPR-2014}. While previous cascade schemes rely on a fixed ordering of evaluations, our scheme learns to traverse the available features/classifiers in arbitrary orders.

\paragraph{Dynamic selection}\label{dynamic-selection}
Most recently, learnt feature and classification deployment schemes based on budget aware policies have shown how to flexibly adapt computation at test time dependent on the input and intermediate results \cite{Karayev-NIPS-2012,karayev14cvpr}. We extend these ideas to the most recent developments of CNNs for visual object detection with a focus on deployment on highly parallel GPU computing hardware. In particular, we effectively combine  dense evaluation of shared, cheap feature with a fine-grained, dynamic execution policy reasoning on window proposals for more expensive features.

