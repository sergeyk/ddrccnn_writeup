%!TEX root=paper.tex
\section{Related Work}\label{sec:related}

\paragraph{Object recognition with CNN}\label{object-recognition-with-cnn}

The recent success of deep convolutional neural networks (CNN) such as Alexnet \cite{Krizhevsky-NIPS-2012} on image classification tasks such as ImageNet \cite{Deng-CVPR-2009} has prompted many attempts to apply these computationally expensive methods to detection \cite{Girshick-CVPR-2014,Zou-BMVC-2014,Simonyan-ICLR-2014,Erhan-CVPR-2014,Sermanet-ICLR-2014,He-ECCV-2014} .

These previous attempts aim at reducing computational cost by reducing the search space of sliding window search by a bottom up window proposal scheme \cite{Girshick-CVPR-2014}, reducing computational overhead by sharing computation between windows to be evaluated \cite{Zou-BMVC-2014,Sermanet-ICLR-2014}, or deriving a CNN-based window sampling scheme \cite{Simonyan-ICLR-2014}.

\paragraph{Cascaded detection}\label{cascaded-detection}
Detection cascades \cite{Viola-IJCV-2004,Felzenszwalb-CVPR-2010} are a well established techniques to condition the computational scheme on intermediate outcomes based on a fixed evaluation order of features/classifiers.
This idea has seen several refinements including sharing computation across cascades \cite{Dollar-ECCV-2012} and the possibility to skip some evaluations \cite{Benbouzid-ICML-2012}.
Recently, a CNN with a cascaded structure for coarse to fine inference of facial feature points has been proposed \cite{Sun-CVPR-2013} as well as a boosted cascade trained on CNN features \cite{Zou-BMVC-2014}. While previous cascade schemes rely on a fixed ordering of evaluations, our scheme learns to traverse the available features/classifiers in arbitrary orders.

\paragraph{Dynamic selection}\label{dynamic-selection}
Most recently, learnt feature and classification deployment schemes based on budget aware policies have shown how to flexibly adapt computation at test time dependent on the input and intermediate results \cite{Karayev-NIPS-2012,DulacArnold-ICLR-2014}.
